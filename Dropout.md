Dropout是在Alexnet提出后被广泛使用，主要用来解决过拟合的问题；

目前对于dropout在train和inference阶段的处理有两个版本：1)假设神经元保留的概率为P，在train阶段对神经元的激活值除以P，inference阶段不做其他操作；2）train阶段按p的概率保留神经元，在inference阶段将激活值乘以p；

这两种方法目的都是一样，保持使用dropout和不使用dropout时的期望保持一致，以单个神经元举个例子说明一下：

如果某个神经元的输出是X，在不使用dropout时，留下的概率是1，那么期望值就是X*1=X；

如果使用dropout，并且神经元保留的概率是p，那么此时的期望值是p*x+(1-p) * 0 = px;

两者的期望值是不同的，所以要么在train的时候将激活值除以p满足期望都为x，要么在inference阶段将激活值乘以p，满足期望值都为px；

##### 两种方法的异同：

首先两种方法在数学上的作用其实我认为是一样的，都是保证期望不变，虽然两种方法最终保持的期望值不同，但我目前不清楚期望值为x和px是否对模型有影响，猜测应该是没有影响的；

不同点在于方法1在inference应该是比方法2更加快，并且使用方法一更符合一般训练的习惯，只用修改train时的参数，inference都保持一致，所以目前采用方法一的应该更为普遍。